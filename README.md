# InstructLab RAFT

## Introduction

The purpose of this repository is to demonstrate how to perform RAFT (Retrieval Augmented Fine Tuning) by using InstructLab & Milvus on RHEL.

**Fine Tuning**: Train Llama-3.2-3B to know about LLaMA 4 by using InstructLab's Synthetic Data Generation.

**Serving**: The fine tuned model is served via vLLM on top of InstructLab.

**Retrieval Augmented Generation**: Integrate organizational context into the fine-tuned model by embedding the document located at rag/DOG.md into Milvus. This document defines appropriate and inappropriate use cases for LLaMA 4 within a dog adoption organization. The Chat bot will answer according to the organizational context.

---
## Demo

This section provides a walkthrough of the above process.

### Prerequisite
```
# The pip requirements for this walkthrough are not documented (WIP)

$ ilab --version
ilab, version 0.25.0

$ python --version
Python 3.11.7
```
### Walkthrough

1. Configure InstructLab. In order to train the Llama-3.2-3B, use at least 4 x Nvidia L4 GPUs. Use the default settings for the InstructLab setup -

```
$ ilab config init
```

2. Download `mistral-7b-instruct-v0.2.Q4_K_M.gguf` and `meta-llama/Llama-3.2-3B` models -

```
$ ilab model download --repository TheBloke/Mistral-7B-Instruct-v0.2-GGUF --filename=mistral-7b-instruct-v0.2.Q4_K_M.gguf --hf-token $HUGGINGFACE_RO_TOKEN

$ ilab model download --repository=meta-llama/Llama-3.2-3B --hf-token $HUGGINGFACE_RO_TOKEN

$ ilab model list
+--------------------------------------+---------------------+----------+-------------------------------------------------------------------------------+
| Model Name                           | Last Modified       | Size     | Absolute path                                                                 |
+--------------------------------------+---------------------+----------+-------------------------------------------------------------------------------+
| meta-llama/Llama-3.2-3B              | 2025-04-20 10:59:54 | 6.0 GB   | /home/instruct/.cache/instructlab/models/meta-llama                           |
| mistral-7b-instruct-v0.2.Q4_K_M.gguf | 2025-04-20 12:40:22 | 4.1 GB   | /home/instruct/.cache/instructlab/models/mistral-7b-instruct-v0.2.Q4_K_M.gguf |
+--------------------------------------+---------------------+----------+-------------------------------------------------------------------------------+
```

3. Add the `sdg/qna.yaml` file into the taxonomy directory structure -
```
$ mkdir -p ~/.local/share/instructlab/taxonomy/knowledge/llms/llama/

$ cp sdg/qna.yaml ~/.local/share/instructlab/taxonomy/knowledge/llms/llama/

$ ilab taxonomy diff
knowledge/llms/llama/qna.yaml

Taxonomy in /home/instruct/.local/share/instructlab/taxonomy is valid :)
```

4. Generate synthetic data by using `ilab generate` -
```
$ ilab data generate --num-instructions 500 --enable-serving-output --gpus 4
```

5. Verify the data generated by InstructLab -
```
cat ~/.local/share/instructlab/datasets/2025-04-20_134524/knowledge_train_msgs_2025-04-20T13_45_48.jsonl  | jq .
. . .
    {
      "content": "What are the key features of Meta's LLaMA 4 release?",
      "role": "user"
    },
    {
      "content": "Key features of Meta's LLaMA 4 release include a context length of 128K tokens, instruction tuning and tool use emphasis, alignment improvements using RLHF, and open research ethos.\n",
      "role": "assistant"
    }
. . .
```

6. Train the LLaMA 3 model based on the generated data -
```
$ ilab model train --model-path ~/.cache/instructlab/models/meta-llama/Llama-3.2-3B --data-path ~/.local/share/instructlab/datasets/2025-04-20_134524/knowledge_train_msgs_2025-04-20T13_45_48.jsonl --device cuda --pipeline accelerated --gpus 4 --num-epochs 15
```

### Test

This section is separated into 3 parts.

* Serve the orgiginal LLaMA 3 model.
* Serve the fine tuned LLaMA 3 model.
* Serve the fine tuned LLaMA 3 model with a Milvus Vector DB for RAG.

1. Serve the original LLaMA 3 model -

```
$ ilab model serve --model-path /home/instruct/.cache/instructlab/models/meta-llama/Llama-3.2-3B/ --gpus=4
```

2. In another window, create an interactive chat with the model, notice that the answers the model provides are clear hallucinations -

```
$ ilab model chat --max-tokens 40
╭───────────────────────────────────────────────────────────────────── system ──────────────────────────────────────────────────────────────────────╮
│ Welcome to InstructLab Chat w/ LLAMA-3.2-3B (type /h for help)                                                                                    │
╰───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
>>> What are the benefits of LLaMA 4?                                                                                                    [S][default]
╭────────────────────────────────────────────────────────────────── Llama-3.2-3B ───────────────────────────────────────────────────────────────────╮
│ LLaMA 4 comes with a new parametric schedule, stability improves significantly, and the LM has been subjected to several extensive head and       │
│ conditional supervision.
```

3. Stop the serving of the previous model, and serve the fine tuned model -

```
$ ilab model serve --gpus 4 --model-path ~/.local/share/instructlab/checkpoints/hf_format/samples_1785/
```

4. In another window, create an interactive chat with the model, notice that the answers the model provides are relevant to the topic -

```
$ ilab model chat --max-tokens 40
╭───────────────────────────────────────────────────────────────────── system ──────────────────────────────────────────────────────────────────────╮
│ Welcome to InstructLab Chat w/ SAMPLES_1785 (type /h for help)                                                                                    │
╰───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
>>> What are the benefits of LLaMA 4?                                                                                                    [S][default]
╭────────────────────────────────────────────────────────────────── samples_1785 ───────────────────────────────────────────────────────────────────╮
│ LLaMA 4 offers several benefits, including advanced capabilities in reasoning, context retention, and multilingual comprehension. It allows for   │
│ more precise and useful translations, as well as improved voice and language recognition
```

5. Take a look at `rag/DOG.md`. The document sets the context for an organization for when to use / not to use LLaMA 4. While serving the fine tuned model, run the python script at `rag/rag.py`. Note that the question asked in the example is - _'Should I use LLaMA 4 for Senior dog adoption programs?'_ -

```
$ python rag.py 

==================================================
Yes, Senior dogs often have unique needs and characteristics, which can be better understood and adapted to by LLaMA 4’s advanced reasoning capabilities. It can process complex language, recognize context,
==================================================
```

6. Change row 29 in DOG.md to the next value. The new value is the opposite of the previous content -
```
| 29 | Senior dog adoption program | No | Using LLMs for senior dogs can be considered dangerous due to potential dog health issues |
```

7. Run the script once again after changing the contents of the doc -
```
$ python rag.py 

==================================================
No, using LLaMA 4 for senior dog adoption programs can be considered dangerous due to potential dog health issues that may require specialized care or guidance from a veterinarian or behaviorist.
==================================================
```